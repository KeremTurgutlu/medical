{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp vision.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement ideas from [kaggle competition](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/117242) which can be used for any sequenced imaging modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from fastai.medical.imaging import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we already have representations for each slice in a sequence, let's start off from AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, sl, dim = 32, 16, 1024\n",
    "x = torch.randn(bs,sl,dim); x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceClassifierA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceClassifierA(Module):\n",
    "    \"Predicts for each sequence, e.g. no pooling\"\n",
    "    def __init__(self, emb_sz, n_hid, n_out, bidir=True, ps=0.3, rnn=nn.LSTM):\n",
    "        self.rnn1 = rnn(emb_sz, n_hid, bidirectional=True)\n",
    "        self.rnn2 = rnn(n_hid*2, n_hid//2, bidirectional=True)\n",
    "        dims = [n_hid, n_hid//2, n_out]\n",
    "        ps = [ps]*len(dims)\n",
    "        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "        layers = [LinBnDrop(i, o, p=p, act=a, bn=False) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "        self.cls_head = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn1(x)\n",
    "        x, _ = self.rnn2(x)\n",
    "        return self.cls_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = SequenceClassifierA(dim, n_hid=256, n_out=1, rnn=nn.GRU)\n",
    "out = seq_model(x).sigmoid()\n",
    "assert out.shape == torch.Size([32,16,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceClassifierB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/darraghdog/rsna/blob/a97018a7b7ec920425189c7e37c1128dd9cb0158/scripts/resnext101v12/trainlstmdeltasum.py#L352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifierB(Module):\n",
    "    def __init__(self, dim, n_hid=64, p = 0.3, n_class=2):\n",
    "        \n",
    "        self.embedding_dropout = SpatialDropout(p)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(dim, n_hid, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(n_hid * 2, n_hid, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(n_hid*2, n_hid*2)\n",
    "        self.linear2 = nn.Linear(n_hid*2, n_hid*2)\n",
    "\n",
    "        self.linear = nn.Linear(n_hid*2, n_class)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        \n",
    "        x_add = torch.cat((x, x), -1)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(x)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        h_conc_linear1  = F.relu(self.linear1(h_lstm1))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_lstm2))\n",
    "        \n",
    "        hidden = h_lstm1 + h_lstm2 + h_conc_linear1 + h_conc_linear2 + x_add\n",
    "\n",
    "        output = self.linear(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = SequenceClassifierB(dim, n_hid=1024, n_class=1)\n",
    "out = seq_model(x).sigmoid()\n",
    "assert out.shape == torch.Size([32,16,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifierB(Module):\n",
    "    def __init__(self, model_num, feature_dim, drop_out, feature_num=128, hidden=96, lstm_layers=2, add_position=True):\n",
    "        # seq model 1\n",
    "        self.fea_conv = nn.Sequential(nn.Dropout2d(drop_out),\n",
    "                                      nn.Conv2d(feature_dim, 512, kernel_size=(1, 1), stride=(1,1),padding=(0,0), bias=False),\n",
    "                                      nn.BatchNorm2d(512),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout2d(drop_out),\n",
    "                                      nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "                                      nn.BatchNorm2d(128),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout2d(drop_out),\n",
    "                                      )\n",
    "\n",
    "        self.fea_first_final = nn.Sequential(nn.Conv2d(128*feature_num, 6, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        fea = self.fea_conv(fea)\n",
    "        fea = fea.permute(0, 1, 3, 2).contiguous()\n",
    "        fea = fea.view(batch_size, 128 * feature_num, -1).contiguous()\n",
    "        fea = fea.view(batch_size, 128 * feature_num, -1, 1).contiguous()\n",
    "        fea_first_final = self.fea_first_final(fea)\n",
    "        #################################################\n",
    "        out0 = fea_first_final.permute(0, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifierB(Module):\n",
    "    def __init__(self, model_num, feature_dim, drop_out, feature_num=128, hidden=96, lstm_layers=2, add_position=True):\n",
    "        # seq model 1\n",
    "        self.fea_conv = nn.Sequential(nn.Dropout2d(drop_out),\n",
    "                                      nn.Conv2d(feature_dim, 512, kernel_size=(1, 1), stride=(1,1),padding=(0,0), bias=False),\n",
    "                                      nn.BatchNorm2d(512),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout2d(drop_out),\n",
    "                                      nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "                                      nn.BatchNorm2d(128),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout2d(drop_out),\n",
    "                                      )\n",
    "\n",
    "        self.fea_first_final = nn.Sequential(nn.Conv2d(128*feature_num, 6, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=True))\n",
    "\n",
    "        # # bidirectional GRU\n",
    "        self.hidden_fea = hidden\n",
    "        self.fea_lstm = nn.GRU(128*feature_num, self.hidden_fea, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fea_lstm_final = nn.Sequential(nn.Conv2d(1, 6, kernel_size=(1, self.hidden_fea*2), stride=(1, 1), padding=(0, 0), dilation=1, bias=True))\n",
    "\n",
    "        ratio = 4\n",
    "        if add_position:\n",
    "            model_num += 2\n",
    "        else:\n",
    "            model_num += 1\n",
    "\n",
    "        # seq model 2\n",
    "        self.conv_first = nn.Sequential(nn.Conv2d(model_num, 128*ratio, kernel_size=(5, 1), stride=(1,1),padding=(2,0),dilation=1, bias=False),\n",
    "                                        nn.BatchNorm2d(128*ratio),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(128*ratio, 64*ratio, kernel_size=(3, 1), stride=(1, 1), padding=(2, 0),dilation=2, bias=False),\n",
    "                                        nn.BatchNorm2d(64*ratio),\n",
    "                                        nn.ReLU())\n",
    "\n",
    "        self.conv_res = nn.Sequential(nn.Conv2d(64 * ratio, 64 * ratio, kernel_size=(3, 1), stride=(1, 1),padding=(4, 0),dilation=4, bias=False),\n",
    "                                      nn.BatchNorm2d(64 * ratio),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv2d(64 * ratio, 64 * ratio, kernel_size=(3, 1), stride=(1, 1),padding=(2, 0),dilation=2, bias=False),\n",
    "                                      nn.BatchNorm2d(64 * ratio),\n",
    "                                      nn.ReLU(),)\n",
    "\n",
    "        self.conv_final = nn.Sequential(nn.Conv2d(64*ratio, 1, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1,bias=False))\n",
    "\n",
    "        # bidirectional GRU\n",
    "        self.hidden = hidden\n",
    "        self.lstm = nn.GRU(64*ratio*6, self.hidden, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.final = nn.Sequential(nn.Conv2d(1, 6, kernel_size=(1, self.hidden*2), stride=(1, 1), padding=(0, 0), dilation=1, bias=True))\n",
    "\n",
    "\n",
    "    def forward(self, fea, x):\n",
    "        batch_size, _, _, _ = x.shape\n",
    "\n",
    "        fea = self.fea_conv(fea)\n",
    "        fea = fea.permute(0, 1, 3, 2).contiguous()\n",
    "        fea = fea.view(batch_size, 128 * feature_num, -1).contiguous()\n",
    "        fea = fea.view(batch_size, 128 * feature_num, -1, 1).contiguous()\n",
    "        fea_first_final = self.fea_first_final(fea)\n",
    "        #################################################\n",
    "        out0 = fea_first_final.permute(0, 3, 2, 1)\n",
    "        #################################################\n",
    "\n",
    "        # bidirectional GRU\n",
    "        fea = fea.view(batch_size, 128 * feature_num, -1).contiguous()\n",
    "        fea = fea.permute(0, 2, 1).contiguous()\n",
    "        fea, _ = self.fea_lstm(fea)\n",
    "        fea = fea.view(batch_size, 1, -1, self.hidden_fea * 2)\n",
    "        fea_lstm_final = self.fea_lstm_final(fea)\n",
    "        fea_lstm_final = fea_lstm_final.permute(0, 3, 2, 1)\n",
    "        #################################################\n",
    "        out0 += fea_lstm_final\n",
    "        #################################################\n",
    "\n",
    "        out0_sigmoid = torch.sigmoid(out0)\n",
    "        x = torch.cat([x, out0_sigmoid], dim = 1)\n",
    "        x = self.conv_first(x)\n",
    "        x = self.conv_res(x)\n",
    "        x_cnn = self.conv_final(x)\n",
    "        #################################################\n",
    "        out = x_cnn\n",
    "        #################################################\n",
    "\n",
    "        # bidirectional GRU\n",
    "        x = x.view(batch_size, 256, -1, 6)\n",
    "        x = x.permute(0,2,1,3).contiguous()\n",
    "        x = x.view(batch_size, x.size()[1], -1).contiguous()\n",
    "        x, _= self.lstm(x)\n",
    "        x = x.view(batch_size, 1, -1, self.hidden*2)\n",
    "        x = self.final(x)\n",
    "        x = x.permute(0,3,2,1)\n",
    "        #################################################\n",
    "        out += x\n",
    "        #################################################\n",
    "        #res\n",
    "        return out, out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = SequenceClassifierB(model_num=5, feature_dim=dim, drop_out=0.5)\n",
    "out = seq_model(x).sigmoid()\n",
    "assert out.shape == torch.Size([32,16,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing\n",
    "class SequenceModel(Module):\n",
    "    def __init__(self, emb_sz, n_hid=64, n_classes):\n",
    "        \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(emb_sz, n_hid, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(n_hid * 2, n_hid, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(n_hid*2, n_hid*2)\n",
    "        self.linear2 = nn.Linear(n_hid*2, n_hid*2)\n",
    "\n",
    "        self.linear = nn.Linear(n_hid*2, n_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "\n",
    "        h_embadd = torch.cat((h_embedding[:,:,:2048], h_embedding[:,:,:2048]), -1)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        h_conc_linear1  = F.relu(self.linear1(h_lstm1))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_lstm2))\n",
    "        \n",
    "        hidden = h_lstm1 + h_lstm2 + h_conc_linear1 + h_conc_linear2 + h_embadd\n",
    "\n",
    "        output = self.linear(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1024, 128, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv(x.permute(0,2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embed_size=trnemb.shape[-1]*3, LSTM_UNITS=64, DO = 0.3):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.embedding_dropout = SpatialDropout(0.0) #DO)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(LSTM_UNITS*2, LSTM_UNITS*2)\n",
    "        self.linear2 = nn.Linear(LSTM_UNITS*2, LSTM_UNITS*2)\n",
    "\n",
    "        self.linear = nn.Linear(LSTM_UNITS*2, n_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = x\n",
    "\n",
    "        h_embadd = torch.cat((h_embedding[:,:,:2048], h_embedding[:,:,:2048]), -1)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        h_conc_linear1  = F.relu(self.linear1(h_lstm1))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_lstm2))\n",
    "        \n",
    "        hidden = h_lstm1 + h_lstm2 + h_conc_linear1 + h_conc_linear2 + h_embadd\n",
    "\n",
    "        output = self.linear(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = AWD_LSTM_2(emb_sz=128, n_hid=1024, n_out=1024, n_layers=1, bidir=True)\n",
    "l1 = nn.Linear(1024, 1024)\n",
    "m2 = AWD_LSTM_2(emb_sz=1024, n_hid=1024, n_out=1024, n_layers=1, bidir=True)\n",
    "l2 = nn.Linear(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
